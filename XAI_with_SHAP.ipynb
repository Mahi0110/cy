```python
# XAI_with_SHAP.ipynb

# 1. Introduction
#    - Purpose: Apply SHAP (SHapley Additive exPlanations) to the trained Autoencoder model
#      to understand feature contributions to anomaly detection.
#    - This notebook assumes that 'Deep_Anomaly_Detection.ipynb' has been run,
#      and a trained autoencoder model ('best_autoencoder_model.keras') is available,
#      along with processed test data and the determined anomaly threshold.

# 2. Setup and Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import shap # SHAP library for explainability
import tensorflow as tf
from tensorflow.keras.models import load_model
import os
import joblib # For loading scalers, encoders, data, threshold

# Define directories
PROCESSED_DATA_DIR = "processed_data"
MODEL_DIR = "models"
XAI_OUTPUT_DIR = "xai_outputs"
os.makedirs(XAI_OUTPUT_DIR, exist_ok=True)

# 3. Load Model, Data, and Key Artifacts
def load_xai_artifacts():
    """Loads the trained model, test data, and other necessary artifacts for XAI."""
    try:
        model_path = os.path.join(MODEL_DIR, 'best_autoencoder_model.keras')
        if not os.path.exists(model_path):
            print(f"Error: Model file not found at {model_path}.")
            return [None]*8
        autoencoder_model = load_model(model_path)
        print(f"Autoencoder model loaded successfully from {model_path}")

        X_test_path = os.path.join(PROCESSED_DATA_DIR, "X_test.csv")
        y_test_encoded_path = os.path.join(PROCESSED_DATA_DIR, "y_test.csv")
        target_le_path = os.path.join(PROCESSED_DATA_DIR, 'target_label_encoder.joblib')
        anomaly_threshold_path = os.path.join(MODEL_DIR, 'anomaly_threshold.joblib')
        X_train_normal_path = os.path.join(PROCESSED_DATA_DIR, "X_train_normal.csv")


        if not all(os.path.exists(p) for p in [X_test_path, y_test_encoded_path, target_le_path, anomaly_threshold_path, X_train_normal_path]):
            print("Error: One or more required artifact files not found (X_test, y_test, target_encoder, threshold, X_train_normal).")
            return [None]*8

        X_test = pd.read_csv(X_test_path)
        y_test_encoded_df = pd.read_csv(y_test_encoded_path)
        y_test_encoded = y_test_encoded_df.squeeze()
        feature_names = X_test.columns.tolist()
        print(f"Test data loaded: X_test shape {X_test.shape}, y_test_encoded shape {y_test_encoded.shape}")

        target_le = joblib.load(target_le_path)
        anomaly_threshold = joblib.load(anomaly_threshold_path)
        print(f"Target encoder and anomaly threshold ({anomaly_threshold:.4f}) loaded.")

        # Background data for SHAP (normal instances)
        background_data = pd.read_csv(X_train_normal_path)
        if len(background_data) > 200: # SHAP can be slow with large backgrounds
            background_data_sample = background_data.sample(n=min(200, len(background_data)), random_state=42)
        else:
            background_data_sample = background_data
        print(f"SHAP background data sample (from X_train_normal) loaded, shape: {background_data_sample.shape}")

        # Calculate MSE for X_test to identify anomalies for explanation
        predictions = autoencoder_model.predict(X_test)
        mse = np.mean(np.power(X_test.values - predictions, 2), axis=1) # .values if X_test is pandas

        return autoencoder_model, X_test, y_test_encoded, mse, anomaly_threshold, feature_names, target_le, background_data_sample

    except Exception as e:
        print(f"Error loading artifacts: {e}")
        return [None]*8 # Return list of Nones matching expected output tuple

# (autoencoder_model, X_test, y_test_encoded,
#  reconstruction_errors_mse, anomaly_threshold,
#  feature_names, target_le, background_data_sample) = load_xai_artifacts()


# 4. Initialize SHAP Explainer
#    - For Keras models, DeepExplainer is efficient but sometimes tricky with complex models or specific TF versions.
#    - KernelExplainer is model-agnostic and more robust but significantly slower.
#    - We want to explain the reconstruction error (MSE).
# if autoencoder_model is not None and background_data_sample is not None:
#     print("\n--- Initializing SHAP Explainer ---")
#     # To explain MSE, we need a function that takes model input and returns MSE.
#     # KernelExplainer is well-suited for this if DeepExplainer struggles.

#     def mse_prediction_function(data_as_numpy_array):
#         """ SHAP prediction function: input is numpy array, output is MSE (numpy array of shape (n_samples,)). """
#         if not isinstance(data_as_numpy_array, np.ndarray):
#             data_as_numpy_array = np.array(data_as_numpy_array)
#
#         # Ensure data_as_numpy_array has the correct number of features if it's 1D for a single instance
#         if data_as_numpy_array.ndim == 1:
#            data_as_numpy_array = data_as_numpy_array.reshape(1, -1)
#
#         reconstructions = autoencoder_model.predict(data_as_numpy_array)
#         mse_values = np.mean(np.power(data_as_numpy_array - reconstructions, 2), axis=1)
#         return mse_values

    # Try DeepExplainer first, as it's faster if it works.
    # DeepExplainer explains the direct output of the model (reconstruction).
    # We can then try to correlate SHAP values for reconstruction with high MSE.
    # This is an indirect way to explain anomalies.
    # explainer_deep = shap.DeepExplainer(autoencoder_model, background_data_sample.values)
    # print("SHAP DeepExplainer initialized for autoencoder reconstruction output.")
    # print("Note: DeepExplainer explains the reconstructed features, not directly the MSE.")
    # print("For direct MSE explanation, KernelExplainer (below, commented out) is an alternative but slower.")

    # KernelExplainer for direct MSE explanation (can be very slow)
    # print("Initializing SHAP KernelExplainer for MSE (this might take a while for the summary plot later)...")
    # explainer_kernel_mse = shap.KernelExplainer(mse_prediction_function, background_data_sample.values)
    # print("SHAP KernelExplainer for MSE initialized.")
    # Use this one for more direct explanation of anomaly score.

    # Choose one explainer to proceed with for generating values. KernelExplainer is more direct for MSE.
    # explainer_to_use = explainer_kernel_mse # or explainer_deep


# 5. Generate SHAP Values
#    - Select some instances to explain (e.g., a few true anomalies and a few normal instances).
# if 'explainer_to_use' in locals() and explainer_to_use is not None and X_test is not None and reconstruction_errors_mse is not None:
#     print("\n--- Generating SHAP Values ---")
#     # Identify actual anomalies and normal instances from X_test based on the threshold
#     anomalies_xtest_idx = np.where(reconstruction_errors_mse > anomaly_threshold)[0]
#     normals_xtest_idx = np.where(reconstruction_errors_mse <= anomaly_threshold)[0]

#     # Take a small sample to explain (e.g., 3 anomalies, 2 normals for speed)
#     # Ensure indices are within bounds
#     sample_anomalies_idx = np.random.choice(anomalies_xtest_idx, size=min(3, len(anomalies_xtest_idx)), replace=False) if len(anomalies_xtest_idx) > 0 else np.array([])
#     sample_normals_idx = np.random.choice(normals_xtest_idx, size=min(2, len(normals_xtest_idx)), replace=False) if len(normals_xtest_idx) > 0 else np.array([])

#     if len(sample_anomalies_idx) == 0 and len(sample_normals_idx) == 0:
#         print("No samples selected for explanation (possibly no anomalies/normals found or empty X_test). Skipping SHAP value generation.")
#     else:
#         instances_to_explain_idx = np.concatenate([sample_anomalies_idx, sample_normals_idx]).astype(int)
#         instances_to_explain_df = X_test.iloc[instances_to_explain_idx]
#         instances_to_explain_numpy = instances_to_explain_df.values

#         print(f"Selected {len(instances_to_explain_idx)} instances for SHAP explanation (indices: {instances_to_explain_idx}).")
#         print(f"Corresponding Reconstruction Errors (MSE): {reconstruction_errors_mse[instances_to_explain_idx]}")

        # Generate SHAP values. This can be very slow for KernelExplainer.
        # For KernelExplainer explaining MSE: shap_values will be (n_samples, n_features)
        # For DeepExplainer explaining reconstruction: shap_values is a list (one per output neuron) or array.
        # If it's DeepExplainer for reconstruction, shap_values_deep[0] might be for the first output feature.
        # We'd typically sum or average contributions if explaining reconstruction.

#         print(f"Generating SHAP values using {type(explainer_to_use).__name__}... (This can take a significant amount of time for KernelExplainer)")
#         shap_values = explainer_to_use.shap_values(instances_to_explain_numpy)
#         print("SHAP values generated.")

        # If using DeepExplainer for reconstruction, shap_values might be a list if model has multiple outputs.
        # For an AE, output is multi-dimensional (reconstruction of input).
        # We might need to sum SHAP values across all output features to get a single importance score per input feature.
        # if isinstance(shap_values, list) and type(explainer_to_use).__name__ == 'DeepExplainer':
        #    shap_values_aggregated = np.sum(np.array(shap_values), axis=0) # Summing over output features
        # else:
        #    shap_values_aggregated = shap_values # Assuming it's already (n_samples, n_features) for KernelExplainer


# 6. Visualize SHAP Explanations
# if 'shap_values_aggregated' in locals() and shap_values_aggregated is not None:
#     print("\n--- Visualizing SHAP Explanations ---")

    # Summary Plot (Global Feature Importance)
    # This shows which features are most important overall for the model's output (MSE or reconstruction).
#     plt.figure()
#     shap.summary_plot(shap_values_aggregated, instances_to_explain_df, feature_names=feature_names, show=False)
#     plt.title(f"SHAP Summary Plot (Explaining {('MSE' if type(explainer_to_use).__name__ == 'KernelExplainer' else 'Reconstruction')})")
#     plt.savefig(os.path.join(XAI_OUTPUT_DIR, "shap_summary_plot.png"), bbox_inches='tight')
#     plt.show()

    # Force plots for individual instances
    # These show how features contributed to the prediction for a single instance.
#     expected_value_to_plot = explainer_to_use.expected_value
#     if isinstance(expected_value_to_plot, np.ndarray) and type(explainer_to_use).__name__ == 'DeepExplainer':
#         expected_value_to_plot = expected_value_to_plot[0] # Common for DeepExplainer with single effective output summed up

#     for i, original_idx in enumerate(instances_to_explain_idx):
#         actual_class_name = target_le.inverse_transform([y_test_encoded.iloc[original_idx]])[0]
#         is_anomaly_text = "Anomaly" if reconstruction_errors_mse[original_idx] > anomaly_threshold else "Normal"
#         plot_title = (f"SHAP Force Plot for Instance {original_idx}\n"
#                       f"Actual: {actual_class_name} ({is_anomaly_text}) - MSE: {reconstruction_errors_mse[original_idx]:.4f}")

#         shap.force_plot(expected_value_to_plot,
#                         shap_values_aggregated[i],
#                         instances_to_explain_df.iloc[i],
#                         feature_names=feature_names,
#                         matplotlib=True, show=False)
#         plt.title(plot_title, fontsize=10)
#         plt.savefig(os.path.join(XAI_OUTPUT_DIR, f"shap_force_plot_instance_{original_idx}.png"), bbox_inches='tight')
#         plt.show()

# 7. Interpretation of SHAP Results (Placeholder)
#    - Discuss which features are most influential for high reconstruction errors (anomalies).
#    - Example: "Features like 'protocol_type_X' or 'high_packet_count' consistently show high SHAP values for anomalous instances,
#      suggesting they are key drivers for the model identifying these as deviations."
#    - Discuss if these make sense in the context of IIoT security.

# 8. Placeholder for User Action Reminder
print("\n--- USER ACTION REQUIRED (after running Deep Anomaly Detection Notebook) ---")
print(f"1. Ensure the trained model ('{os.path.join(MODEL_DIR, 'best_autoencoder_model.keras')}'), "
      f"processed test data, target encoder, anomaly threshold, and X_train_normal.csv are available.")
print(f"2. Choose which SHAP explainer to use by uncommenting either `explainer_deep` or `explainer_kernel_mse` related lines, and setting `explainer_to_use`.")
print(f"   - `KernelExplainer` directly explains MSE but is VERY SLOW.")
print(f"   - `DeepExplainer` explains reconstruction; interpreting its SHAP values for MSE is less direct but faster.")
print(f"3. Uncomment the code blocks cell by cell and run them. Be patient if using KernelExplainer.")
print(f"4. Analyze the generated SHAP plots to understand feature importance for detected anomalies.")
print("-----------------------------------------------------------------------------")

# 9. Conclusion
#    - Summary of XAI findings (once generated).
#    - How these explanations can aid in understanding and trusting the anomaly detection model.
```
