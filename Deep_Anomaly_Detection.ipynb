```python
# Deep_Anomaly_Detection.ipynb

# 1. Introduction
#    - Purpose: Implement, train, and evaluate an Autoencoder (AE) or Variational Autoencoder (VAE)
#      for anomaly detection on the preprocessed IIoT dataset.
#    - This notebook assumes that 'IIoT_Dataset_EDA_Preprocessing.ipynb' has been run
#      and the processed data (X_train_normal.csv, X_test.csv, y_test.csv) is available.

# 2. Setup and Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split # Might be needed if further splitting X_train_normal
from sklearn.preprocessing import MinMaxScaler # Or StandardScaler, ensure consistency with preprocessing
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve

import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import os
import joblib # For loading scalers/encoders if needed for inverse transform or interpretation

# Define directories
PROCESSED_DATA_DIR = "processed_data"
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

# Parameters for the Autoencoder
INPUT_DIM = None # To be set after loading X_train_normal
ENCODING_DIM = 64 # Example: Latent space dimension, can be tuned
# Consider making ENCODING_DIM dynamic, e.g., INPUT_DIM / 4 or similar, but fixed for now.
EPOCHS = 100
BATCH_SIZE = 32

# 3. Load Processed Data
#    - We need X_train_normal (containing only normal data for AE training)
#    - And X_test, y_test_encoded (containing normal and anomalous data for evaluation)
def load_ae_data():
    """Loads preprocessed data for Autoencoder training and testing."""
    try:
        x_train_normal_path = os.path.join(PROCESSED_DATA_DIR, "X_train_normal.csv")
        if not os.path.exists(x_train_normal_path):
            print(f"Error: {x_train_normal_path} not found.")
            print("Please ensure 'IIoT_Dataset_EDA_Preprocessing.ipynb' was run successfully and 'X_train_normal.csv' was saved.")
            return None, None, None, None

        X_train_normal = pd.read_csv(x_train_normal_path)
        print(f"Loaded X_train_normal from {x_train_normal_path}, shape: {X_train_normal.shape}")

        X_test = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, "X_test.csv"))
        # y_test.csv contains the original labels, possibly multi-class, encoded.
        y_test_encoded_df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, "y_test.csv"))
        y_test_encoded = y_test_encoded_df.squeeze() # Convert to Series if it's a single column DF

        # Load target encoder to identify the 'normal' class label for binary evaluation
        target_le_path = os.path.join(PROCESSED_DATA_DIR, 'target_label_encoder.joblib')
        if not os.path.exists(target_le_path):
            print(f"Error: {target_le_path} not found. Cannot determine 'normal' class for binary evaluation.")
            return None, None, None, None
        target_le = joblib.load(target_le_path)

        print(f"X_test shape: {X_test.shape}, y_test_encoded shape: {y_test_encoded.shape}")

        global INPUT_DIM
        INPUT_DIM = X_train_normal.shape[1]
        print(f"Input dimension for Autoencoder set to: {INPUT_DIM}")

        return X_train_normal, X_test, y_test_encoded, target_le

    except FileNotFoundError as e:
        print(f"Error: A processed data file not found. {e}")
        print("Please ensure 'IIoT_Dataset_EDA_Preprocessing.ipynb' was run successfully and all files were saved.")
        return None, None, None, None
    except Exception as e:
        print(f"Error loading processed data: {e}")
        return None, None, None, None

# X_train_normal, X_test, y_test_encoded, target_le = load_ae_data()

# 4. Define Autoencoder (AE) Model
def create_autoencoder(input_dim, encoding_dim_ratio=0.25, intermediate_ratio=0.5):
    """
    Creates a simple Autoencoder model.
    encoding_dim_ratio: Ratio of input_dim for bottleneck layer size.
    intermediate_ratio: Ratio of input_dim for intermediate encoder/decoder layers.
    """
    actual_encoding_dim = max(2, int(input_dim * encoding_dim_ratio)) # Ensure at least 2
    intermediate_dim = max(actual_encoding_dim, int(input_dim * intermediate_ratio)) # Ensure intermediate is not smaller than encoding

    print(f"Autoencoder architecture: Input({input_dim}) -> Dense({intermediate_dim}) -> Dense({actual_encoding_dim}) -> Dense({intermediate_dim}) -> Dense({input_dim})")

    input_layer = Input(shape=(input_dim,))

    # Encoder
    encoder = Dense(intermediate_dim, activation='relu')(input_layer)
    encoder = Dense(actual_encoding_dim, activation='relu')(encoder) # Bottleneck

    # Decoder
    decoder = Dense(intermediate_dim, activation='relu')(encoder)
    # Output layer: 'sigmoid' if inputs are scaled [0,1], 'linear' or other if inputs scaled differently (e.g. StandardScaler)
    # Assuming StandardScaler was used in preprocessing, so linear activation might be more appropriate or relu.
    # However, sigmoid is common for reconstruction tasks if data can be somewhat bounded or normalized to positive.
    # Let's stick to 'relu' for hidden layers and 'sigmoid' for output assuming data is somewhat normalized positive by preprocessing.
    # If issues arise, this is a key point for tuning.
    decoder = Dense(input_dim, activation='sigmoid')(decoder)

    autoencoder = Model(inputs=input_layer, outputs=decoder)
    autoencoder.compile(optimizer='adam', loss='mean_squared_error')

    print("\nAutoencoder Model Summary:")
    autoencoder.summary()
    return autoencoder

# if INPUT_DIM is not None:
#    ae_model = create_autoencoder(INPUT_DIM) # Use default ratios

# 5. Train the Autoencoder Model
# if X_train_normal is not None and 'ae_model' in locals() and ae_model is not None:
#     print("\n--- Training Autoencoder ---")
#     # Data for AE should be scaled, typically to [0,1] for sigmoid output layer.
#     # The preprocessing notebook should handle scaling. If it used StandardScaler,
#     # MinMaxScaler might be needed here just for AE training, or AE output activation changed.
#     # For simplicity, assume preprocessing output is suitable for sigmoid (e.g. features are mostly positive and scaled).
#     X_train_normal_fit = X_train_normal.copy()
#
#     model_checkpoint_path = os.path.join(MODEL_DIR, 'best_autoencoder_model.keras')
#     early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
#     model_checkpoint = ModelCheckpoint(model_checkpoint_path, save_best_only=True, monitor='val_loss', verbose=1)
#
#     history = ae_model.fit(X_train_normal_fit, X_train_normal_fit,
#                            epochs=EPOCHS,
#                            batch_size=BATCH_SIZE,
#                            shuffle=True,
#                            validation_split=0.1,
#                            callbacks=[early_stopping, model_checkpoint],
#                            verbose=1)
#
#     print(f"Autoencoder training complete. Best model saved to {model_checkpoint_path}")
#
#     plt.figure(figsize=(10, 6))
#     plt.plot(history.history['loss'], label='Train Loss')
#     plt.plot(history.history['val_loss'], label='Validation Loss')
#     plt.title('Model Loss During Training')
#     plt.ylabel('Loss (MSE)')
#     plt.xlabel('Epoch')
#     plt.legend(loc='upper right')
#     plt.savefig(os.path.join(MODEL_DIR, 'autoencoder_training_loss.png'))
#     plt.show()
#
#     ae_model = load_model(model_checkpoint_path) # Load the best model

# 6. Evaluate Anomaly Detection Performance
# if X_test is not None and y_test_encoded is not None and target_le is not None and 'ae_model' in locals() and ae_model is not None:
#     print("\n--- Evaluating Anomaly Detection Performance ---")
#     predictions = ae_model.predict(X_test)
#     mse = np.mean(np.power(X_test.values - predictions, 2), axis=1) # Use .values if X_test is pandas
#     reconstruction_errors_df = pd.DataFrame({'reconstruction_error': mse, 'true_class_encoded': y_test_encoded})
#
#     reconstruction_errors_df['true_class_name'] = target_le.inverse_transform(reconstruction_errors_df['true_class_encoded'])
#
#     normal_class_name_original = None
#     for i, class_name_iter in enumerate(target_le.classes_):
#         if 'normal' in class_name_iter.lower(): # Attempt to find the normal class name
#             normal_class_name_original = class_name_iter
#             break
#     if normal_class_name_original is None:
#         print("Warning: Could not automatically determine the 'normal' class name from encoder. Using first class as 'normal' for binary metrics.")
#         normal_class_name_original = target_le.classes_[0] # Fallback, may be incorrect
#
#     print(f"Assumed 'normal' class for binary evaluation: '{normal_class_name_original}'")
#     normal_class_encoded_val = target_le.transform([normal_class_name_original])[0]
#
#     plt.figure(figsize=(12, 7))
#     for class_name_val in reconstruction_errors_df['true_class_name'].unique():
#         sns.histplot(reconstruction_errors_df[reconstruction_errors_df['true_class_name'] == class_name_val]['reconstruction_error'],
#                      label=class_name_val, kde=True, element="step" if class_name_val != normal_class_name_original else "bars",
#                      alpha=0.7 if class_name_val != normal_class_name_original else 1.0)
#     plt.title('Reconstruction Error Distribution by Class')
#     plt.xlabel('Mean Squared Error (Reconstruction Error)')
#     plt.ylabel('Density')
#     plt.legend()
#     plt.savefig(os.path.join(MODEL_DIR, 'reconstruction_error_distribution.png'))
#     plt.show()
#
#     y_test_binary = (reconstruction_errors_df['true_class_encoded'] != normal_class_encoded_val).astype(int) # 1 for anomaly, 0 for normal
#
#     precision, recall, thresholds_pr = precision_recall_curve(y_test_binary, mse)
#     # Remove cases where precision and recall are both zero to avoid nan in F1 score
#     valid_indices = ~((precision == 0) & (recall == 0))
#     precision_valid = precision[valid_indices]
#     recall_valid = recall[valid_indices]
#     thresholds_pr_valid = thresholds_pr[valid_indices[:-1]] # thresholds_pr is one element longer
#
#     if len(precision_valid) > 0 and len(recall_valid) > 0 : # Ensure there are valid scores
#        f1_scores = np.nan_to_num((2 * precision_valid * recall_valid) / (precision_valid + recall_valid))
#        if len(f1_scores) > 0:
#            best_threshold_idx = np.argmax(f1_scores)
#            best_threshold = thresholds_pr_valid[best_threshold_idx]
#            print(f"Best threshold based on F1 score: {best_threshold}")
#            print(f"Corresponding F1: {f1_scores[best_threshold_idx]:.4f}, Precision: {precision_valid[best_threshold_idx]:.4f}, Recall: {recall_valid[best_threshold_idx]:.4f}")
#        else:
#            best_threshold = np.median(mse) # Fallback threshold
#            print(f"Could not determine best threshold from F1, using median MSE: {best_threshold}")
#     else:
#        best_threshold = np.median(mse) # Fallback threshold
#        print(f"Precision/Recall arrays empty after filtering, using median MSE for threshold: {best_threshold}")
#
#
#     plt.figure(figsize=(8,6))
#     if len(precision_valid) > 0 and len(recall_valid) > 0 and len(f1_scores) > 0:
#         plt.plot(thresholds_pr_valid, precision_valid[:-1] if len(precision_valid)>len(f1_scores) else precision_valid, label="Precision")
#         plt.plot(thresholds_pr_valid, recall_valid[:-1] if len(recall_valid)>len(f1_scores) else recall_valid, label="Recall")
#         plt.plot(thresholds_pr_valid, f1_scores, label="F1-Score", linestyle='--')
#         plt.axvline(best_threshold, color='red', linestyle=':', label=f'Best Threshold ({best_threshold:.4f})')
#     else:
#         plt.text(0.5, 0.5, "Could not plot P-R curve (empty arrays)", ha='center', va='center')
#     plt.title('Precision, Recall, and F1 vs. Threshold')
#     plt.xlabel('Threshold')
#     plt.ylabel('Score')
#     plt.legend()
#     plt.savefig(os.path.join(MODEL_DIR, 'precision_recall_threshold.png'))
#     plt.show()
#
#     y_pred_binary = (mse > best_threshold).astype(int)
#     binary_target_names = [f'Normal ({normal_class_name_original})', 'Anomaly']
#     print("\nClassification Report (Binary: Normal vs Anomaly):")
#     print(classification_report(y_test_binary, y_pred_binary, target_names=binary_target_names))
#
#     cm = confusion_matrix(y_test_binary, y_pred_binary)
#     plt.figure(figsize=(6,5))
#     sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=binary_target_names, yticklabels=binary_target_names)
#     plt.title('Confusion Matrix (Normal vs Anomaly)')
#     plt.ylabel('Actual Class')
#     plt.xlabel('Predicted Class')
#     plt.savefig(os.path.join(MODEL_DIR, 'confusion_matrix_binary.png'))
#     plt.show()
#
#     # Save the determined threshold
#     joblib.dump(best_threshold, os.path.join(MODEL_DIR, 'anomaly_threshold.joblib'))
#     print(f"Anomaly threshold saved to {os.path.join(MODEL_DIR, 'anomaly_threshold.joblib')}")


# 7. Placeholder for User Action Reminder
print("\n--- USER ACTION REQUIRED (after running Preprocessing Notebook) ---")
print(f"1. Ensure '{PROCESSED_DATA_DIR}/X_train_normal.csv', '{PROCESSED_DATA_DIR}/X_test.csv', "
      f"'{PROCESSED_DATA_DIR}/y_test.csv', and '{PROCESSED_DATA_DIR}/target_label_encoder.joblib' exist.")
print(f"2. Review Autoencoder parameters (ENCODING_DIM_RATIO, EPOCHS, BATCH_SIZE) and model architecture if needed.")
print(f"3. The 'normal' class identification for binary evaluation relies on finding 'normal' (case-insensitive) in class names. Verify this or adjust logic if your 'normal' class is named differently.")
print(f"4. Uncomment the code blocks cell by cell and run them.")
print("--------------------------------------------------------------------")

# 8. Next Steps
#    - Phase 3: Explainable AI (XAI) with SHAP, using the trained autoencoder.
```
