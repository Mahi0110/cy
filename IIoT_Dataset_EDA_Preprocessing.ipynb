```python
# IIoT_Dataset_EDA_Preprocessing.ipynb

# 1. Introduction
#    - Purpose of this notebook: Load, explore, and preprocess the new IIoT dataset.
#    - Assumed Dataset: "Edge-IIoTset" (or a similar IIoT dataset provided by the user).
#    - Brief mention of expected features (based on previous simulation).

# 2. Setup and Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder # OneHotEncoder might be added later
from sklearn.impute import SimpleImputer
import os # For saving processed data
import joblib # For saving encoders/scalers

# Placeholder for dataset path - user will need to provide this
DATASET_PATH = "path/to/your/iiot_dataset.csv" # <--- USER ACTION REQUIRED
# Example: DATASET_PATH = "./datasets/Edge-IIoTset.csv"

# Define a directory to save processed data and models/encoders
PROCESSED_DATA_DIR = "processed_data"
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)

# 3. Data Loading
#    - Function to load the dataset (e.g., from CSV)
def load_iiot_data(path):
    """Loads the IIoT dataset from a specified CSV path."""
    try:
        df = pd.read_csv(path)
        print(f"Dataset loaded successfully from {path}")
        print(f"Shape of dataset: {df.shape}")
        print("First 5 rows of the dataset:")
        print(df.head())
        return df
    except FileNotFoundError:
        print(f"Error: Dataset file not found at {path}. Please provide the correct path and ensure the file is accessible.")
        return None
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return None

# df_raw = load_iiot_data(DATASET_PATH) # This will be uncommented and run when dataset is available

# 4. Initial Data Exploration (EDA)
#    - This section will be filled in once df_raw is loaded.
#    - It will include:
#        - df_raw.info()
#        - df_raw.describe(include='all')
#        - df_raw.isnull().sum() and visualization of missing values
#        - Identification of TARGET_COLUMN (e.g., 'attack_label' or 'Attack_Type')
#        - Value counts and distribution of the TARGET_COLUMN
#        - Identification of numerical_features and categorical_features
#        - Histograms for numerical features
#        - Count plots for categorical features
#        - Correlation heatmap for numerical features

# Example (to be run when df_raw is available):
# if df_raw is not None:
#     print("\n--- Initial Data Exploration (EDA) ---")
#     print("\nDataset Info:")
#     df_raw.info()
#     print("\nSummary Statistics:")
#     print(df_raw.describe(include='all'))
#     print("\nMissing values per column:")
#     missing_values = df_raw.isnull().sum()
#     print(missing_values[missing_values > 0]) # Print only columns with missing values
#     # TARGET_COLUMN = 'label' # <--- USER ACTION: Confirm this based on your dataset
#     # print(f"\nDistribution of target variable ('{TARGET_COLUMN}'):")
#     # print(df_raw[TARGET_COLUMN].value_counts())


# 5. Data Preprocessing Strategy & Implementation
#    - This section will be filled in once df_raw is loaded and EDA is done.
#    - It will include:
#        - Handling missing values (imputation)
#        - Encoding categorical features (e.g., LabelEncoding, OneHotEncoding for low cardinality, or more advanced for IPs)
#        - Encoding the target variable
#        - Feature scaling for numerical features
#        - Splitting data into training and testing sets

# Example (to be run when df_raw is available and EDA insights are gathered):
# if df_raw is not None:
#     print("\n--- Data Preprocessing ---")
#     df_processed = df_raw.copy()

#     # Define TARGET_COLUMN (user needs to confirm) and feature types
#     # TARGET_COLUMN = 'label'
#     # numerical_features = df_processed.select_dtypes(include=np.number).columns.tolist()
#     # if TARGET_COLUMN in numerical_features: numerical_features.remove(TARGET_COLUMN)
#     # categorical_features = df_processed.select_dtypes(include='object').columns.tolist()
#     # if TARGET_COLUMN in categorical_features: categorical_features.remove(TARGET_COLUMN)
#     # print(f"Identified Numerical Features: {numerical_features}")
#     # print(f"Identified Categorical Features: {categorical_features}")

#     # 5.1. Handle Missing Values (Example: Median for numerical, Mode for categorical)
#     # for col in numerical_features:
#     #     if df_processed[col].isnull().any():
#     #         median_val = df_processed[col].median()
#     #         df_processed[col].fillna(median_val, inplace=True)
#     # for col in categorical_features:
#     #     if df_processed[col].isnull().any():
#     #         mode_val = df_processed[col].mode()[0]
#     #         df_processed[col].fillna(mode_val, inplace=True)
#     # print("Missing values handled (check df_processed.isnull().sum().sum()).")

#     # 5.2. Encode Categorical Features (Example: Label Encoding)
#     # This is a simplified example. IP addresses, high cardinality features might need specific strategies.
#     # feature_label_encoders = {}
#     # for col in categorical_features:
#     #     le = LabelEncoder()
#     #     df_processed[col] = le.fit_transform(df_processed[col].astype(str)) # astype(str) for safety
#     #     feature_label_encoders[col] = le
#     # joblib.dump(feature_label_encoders, os.path.join(PROCESSED_DATA_DIR, 'feature_label_encoders.joblib'))
#     # print("Categorical features label encoded and encoders saved.")

#     # 5.3. Encode Target Variable
#     # target_le = LabelEncoder()
#     # df_processed[TARGET_COLUMN] = target_le.fit_transform(df_processed[TARGET_COLUMN])
#     # joblib.dump(target_le, os.path.join(PROCESSED_DATA_DIR, 'target_label_encoder.joblib'))
#     # print(f"Target variable '{TARGET_COLUMN}' encoded and encoder saved. Classes: {target_le.classes_}")

#     # 5.4. Feature Scaling (Numerical Features)
#     # scaler = StandardScaler()
#     # df_processed[numerical_features] = scaler.fit_transform(df_processed[numerical_features])
#     # joblib.dump(scaler, os.path.join(PROCESSED_DATA_DIR, 'feature_scaler.joblib'))
#     # print("Numerical features scaled using StandardScaler and scaler saved.")

#     # 5.5. Split Data & Save X_train_normal
#     # X = df_processed.drop(TARGET_COLUMN, axis=1)
#     # y = df_processed[TARGET_COLUMN] # This is the encoded target
#     # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y if y.nunique() > 1 else None)
#     # print(f"Data split: X_train {X_train.shape}, X_test {X_test.shape}, y_train {y_train.shape}, y_test {y_test.shape}")
#
#     # Create and save X_train_normal (critical for Autoencoder training)
#     # This assumes 'target_le' is your fitted LabelEncoder for the target variable,
#     # and you know which original class name corresponds to "normal".
#     # normal_class_name_original = 'Normal' # <--- USER ACTION: Confirm your dataset's name for normal traffic
#     # normal_class_encoded = target_le.transform([normal_class_name_original])[0]
#     # X_train_normal = X_train[y_train == normal_class_encoded]
#     # X_train_normal.to_csv(os.path.join(PROCESSED_DATA_DIR, "X_train_normal.csv"), index=False)
#     # print(f"X_train_normal (only normal samples from training set) saved. Shape: {X_train_normal.shape}")

#     # Save other processed data splits
#     # X_train.to_csv(os.path.join(PROCESSED_DATA_DIR, "X_train.csv"), index=False)
#     # X_test.to_csv(os.path.join(PROCESSED_DATA_DIR, "X_test.csv"), index=False)
#     # y_train_df = pd.DataFrame(y_train, columns=[TARGET_COLUMN]) # Save as DataFrame to preserve column name
#     # y_test_df = pd.DataFrame(y_test, columns=[TARGET_COLUMN])
#     # y_train_df.to_csv(os.path.join(PROCESSED_DATA_DIR, "y_train.csv"), index=False)
#     # y_test_df.to_csv(os.path.join(PROCESSED_DATA_DIR, "y_test.csv"), index=False)
#     # print(f"Processed data splits (X_train, X_test, y_train, y_test) saved to '{PROCESSED_DATA_DIR}'.")

# 6. Placeholder for User Action Reminder
print("\n--- USER ACTION REQUIRED ---")
print(f"1. Place your IIoT dataset (e.g., CSV file) in a known location.")
print(f"2. Update the 'DATASET_PATH' variable in this notebook (cell under 'Setup and Imports').")
print(f"3. Uncomment the 'df_raw = load_iiot_data(DATASET_PATH)' line.")
print(f"4. Run the notebook cell by cell. When you reach EDA and Preprocessing sections:")
print(f"   - Confirm the 'TARGET_COLUMN' name based on your dataset's columns.")
print(f"   - Confirm the 'normal_class_name_original' in section 5.5 for creating 'X_train_normal.csv'.")
print(f"   - Review the identified numerical and categorical features.")
print(f"   - Adapt the missing value handling and encoding strategies if necessary.")
print(f"   - Uncomment and run the code blocks for EDA and Preprocessing.")
print("----------------------------")

# 7. Conclusion & Next Steps
#    - Summary of preprocessing to be performed (once implemented).
#    - The processed data will be ready for model training (Phase 2).
```
